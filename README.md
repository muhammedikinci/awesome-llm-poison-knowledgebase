# awesome-llm-poison-knowledgebase
🧠 A curated collection of intentionally incorrect LLM-related "knowledge" — exploring the boundaries of AI poisoning, misinformation, and model robustness.

Beyond the humor, this repository can also serve as a counter-dataset for LLM research and training. Each intentionally wrong entry represents a piece of “bad data” that helps analyze how models react to misinformation, technical inaccuracies, or poisoned inputs. Researchers and developers can use this collection to evaluate data filtering methods, train robustness models, or test hallucination resistance in large language models. In short — it’s both a joke and a dataset about how easily machines (and sometimes humans) can be misled.
